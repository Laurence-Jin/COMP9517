{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241fbf9d",
   "metadata": {},
   "source": [
    "# Dataset processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e714f34",
   "metadata": {},
   "source": [
    "## I use the dataset from https://motchallenge.net/data/MOT16/ , because it contains our train images with det.txt files. The det.txt files can be converted to YOLO.txt for detecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d811249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "136c1703",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ebcdd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\jupyterdoc\\\\yolov5-6.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d43a7914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(imgWidth, imgHeight, left, top, width, height):\n",
    "    x = (left + width / 2.0) / imgWidth\n",
    "    y = (top + height / 2.0) / imgHeight\n",
    "    w = width / imgWidth\n",
    "    h = height / imgHeight\n",
    "    return ('%.6f'%x, '%.6f'%y, '%.6f'%w, '%.6f'%h) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec60adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_and_label(filepath,new_filepath,namedoc,seqLength,imgWidth,imgHeight):\n",
    "    det_path = filepath + 'det/det.txt'\n",
    "    dets = np.loadtxt(det_path,delimiter=',')\n",
    "    for det in dets:\n",
    "        frame_id, _, left, top, width, height = int(det[0]), det[1], det[2], det[3], det[4], det[5]\n",
    "        box = convert(imgWidth, imgHeight, left, top, width, height)\n",
    "        if '-' in ''.join(box) or float(box[0]) > 1.0 or float(box[1]) > 1.0 or float(box[2]) > 1.0 or float(box[3]) > 1.0:\n",
    "            continue\n",
    "        image_name = namedoc + '-' + '%06d'%frame_id + '.jpg' \n",
    "        label_name = namedoc + '-' + '%06d'%frame_id + '.txt' \n",
    "        oldimgpath = filepath + 'img1/'+ '%06d'%frame_id + '.jpg' \n",
    "        if frame_id <= seqLength // 2: \n",
    "            newimgpath = new_filepath + 'images/train/'+ image_name \n",
    "            labelpath = new_filepath + 'labels/train/' + label_name \n",
    "        else: \n",
    "            newimgpath = new_filepath + 'images/val/'+ image_name \n",
    "            labelpath = new_filepath + 'labels/val/'+ label_name \n",
    "        shutil.copyfile(str(oldimgpath), str(newimgpath)) \n",
    "        with open(labelpath, 'a') as f: \n",
    "            f.write(f'0 {box[0]} {box[1]} {box[2]} {box[3]}\\n')\n",
    "    print('Dataset has processed')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69d91bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has processed\n"
     ]
    }
   ],
   "source": [
    "get_image_and_label(root+'/MOT16/train/MOT16-09/',root+ '/VOCdevkit/','test_1',525,1920,1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "290f13cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has processed\n"
     ]
    }
   ],
   "source": [
    "get_image_and_label(root+'/MOT16/train/MOT16-02/',root+ '/VOCdevkit/','test_2',600,1920,1080)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b9bc33",
   "metadata": {},
   "source": [
    "# use anaconda prompt to run (python train.py --batch 8 --epoch 10) to get our weight file 'Best.pt'. You can directly use the weight file to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c137e10d",
   "metadata": {},
   "source": [
    "<img src = './input_order.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56333c4",
   "metadata": {},
   "source": [
    "<img src = './train_yolo.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e844d0b",
   "metadata": {},
   "source": [
    "# Use YOLO with Best.pt and person.yaml to detect pedestrains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3329fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Ipynb_importer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f61e5f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import compare_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af0d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86972df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.common import DetectMultiBackend\n",
    "from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
    "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
    "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
    "from utils.plots import Annotator, colors, save_one_box\n",
    "from utils.torch_utils import select_device, time_sync\n",
    " \n",
    "#导入letterbox\n",
    "from utils.augmentations import Albumentations, augment_hsv, copy_paste, letterbox, mixup, random_perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50e646ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c97e34d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\jupyterdoc\\\\yolov5-6.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1368575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2022-2-22 torch 1.12.0 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7053910 parameters, 0 gradients\n",
      "WARNING: --img-size (640, 640) must be multiple of max stride 32, updating to [640, 640]\n"
     ]
    }
   ],
   "source": [
    "# This part of the code is quoted from detect.py in YOLOV5, the link is https://github.com/ultralytics/yolov5/releases/tag/v6.1\n",
    "# and I have rewritten this part of the code so that it can return the coordinates of the pedestrian border\n",
    "# I rewrite the detect.py as a function to return (x,y,w,h) \n",
    "weights= root + '/runs/train/exp1/weights/best.pt'  \n",
    "source=root + '/data/images'  \n",
    "data= root + '/data/person.yaml'  \n",
    " \n",
    "imgsz=(640, 640) \n",
    "conf_thres=0.25  \n",
    "iou_thres=0.45  \n",
    "max_det=1000  \n",
    "device='0'  \n",
    "classes=None  \n",
    "agnostic_nms=False  \n",
    "augment=False  \n",
    "visualize=False  \n",
    "half=False  \n",
    "dnn=False  \n",
    " \n",
    " \n",
    "\n",
    "device = select_device(device)\n",
    " \n",
    "\n",
    "model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data)\n",
    "stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
    "imgsz = check_img_size(imgsz, s=stride)  \n",
    " \n",
    "# Half\n",
    "\n",
    "half &= (pt or jit or onnx or engine) and device.type != 'cpu'  # FP16 supported on limited backends with CUDA\n",
    "if pt or jit:\n",
    "    model.model.half() if half else model.model.float()\n",
    " \n",
    " \n",
    "def detect(img):\n",
    "    # Dataloader\n",
    "    \n",
    "    dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt)\n",
    " \n",
    "    # Run inference\n",
    "    \n",
    "    model.warmup(imgsz=(1, 3, *imgsz), half=half)  # warmup\n",
    "    dt, seen = [0.0, 0.0, 0.0], 0\n",
    " \n",
    "    \n",
    "    im0 = img\n",
    "    # Padded resize\n",
    "    im = letterbox(im0, imgsz, stride, auto=pt)[0]\n",
    "    # Convert\n",
    "    im = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "    im = np.ascontiguousarray(im)\n",
    "    t1 = time_sync()\n",
    "    im = torch.from_numpy(im).to(device)\n",
    "    im = im.half() if half else im.float()  # uint8 to fp16/32\n",
    "    im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "    if len(im.shape) == 3:\n",
    "        im = im[None]  # expand for batch dim\n",
    "    t2 = time_sync()\n",
    "    dt[0] += t2 - t1\n",
    " \n",
    "    # Inference\n",
    "    \n",
    "    pred = model(im, augment=augment, visualize=visualize)\n",
    "    t3 = time_sync()\n",
    "    dt[1] += t3 - t2\n",
    " \n",
    "    \n",
    "    pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "    dt[2] += time_sync() - t3\n",
    " \n",
    "    \n",
    "    detections=[]\n",
    "    \n",
    "    # Process predictions\n",
    "    for i, det in enumerate(pred):  # per image \n",
    "        seen += 1\n",
    "       \n",
    "        if len(det):\n",
    "            # Rescale boxes from img_size to im0 size\n",
    "            det[:, :4] = scale_coords(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "            # Write results\n",
    "            \n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4))).view(-1).tolist()\n",
    "                xywh = [round(x) for x in xywh]\n",
    "                xywh = [xywh[0] - xywh[2] // 2, xywh[1] - xywh[3] // 2, xywh[2],\n",
    "                        xywh[3]]  # （left，top，w，h）\n",
    " \n",
    "                cls = names[int(cls)]\n",
    "                conf = float(conf)\n",
    "                detections.append({'position': xywh})\n",
    "    \n",
    "    return detections\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e250aae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1621, 440, 50, 169]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = root + '/test/1/000001.jpg'\n",
    "img = cv2.imread(path)\n",
    "\n",
    "\n",
    "detect(img)[0]['position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95be9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = detect(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11a2cb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29b1e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filepath):\n",
    "    file_dir = os.path.join(filepath)\n",
    "    if os.path.exists(file_dir):\n",
    "        files = os.listdir(file_dir)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850fc142",
   "metadata": {},
   "source": [
    "# If you want to test your own images, you can load your images in test/1 folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d46fa916",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_path = root + '/test/1/'\n",
    "test_2_path = root + '/test/2/'\n",
    "test_1 = get_data(test_1_path)\n",
    "test_2 = get_data(test_2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bb9633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pedestrains position in i-th image \n",
    "def cal_img(train_path,train,i):\n",
    "    vlist = []\n",
    "    path = train_path + train[i]\n",
    "    img = cv2.imread(path)\n",
    "    save = detect(img)\n",
    "    for j in range(len(save)):\n",
    "        vlist.append(save[j]['position'])\n",
    "    return vlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af550edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMS(boxes, threshold):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    boxes = np.array(boxes).astype(\"float\")\n",
    "\n",
    "    x1 = boxes[:,0]  \n",
    "    y1 = boxes[:,1]\n",
    "    w1 = boxes[:,2]  \n",
    "    h1 = boxes[:,3]  \n",
    "    x2 = x1 + w1  \n",
    "    y2 = y1 + h1  \n",
    "    \n",
    "    area = (w1 + 1) * (h1 + 1)  \n",
    "    temp = []\n",
    "    \n",
    "    idxs = np.argsort(h1)\n",
    "    \n",
    "    while len(idxs) > 0:\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        temp.append(i)   \n",
    "        \n",
    "        x1_m = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        y1_m = np.maximum(y1[i], y1[idxs[:last]])  \n",
    "        \n",
    "        x2_m = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        y2_m = np.minimum(y2[i], y2[idxs[:last]])\n",
    "       \n",
    "\n",
    "        w = np.maximum(0, x2_m - x1_m + 1)\n",
    "        h = np.maximum(0, y2_m - y1_m + 1)\n",
    "       \n",
    "        over = (w * h) / area[idxs[:last]]\n",
    "        \n",
    "        idxs = np.delete(idxs, np.concatenate(([last],  \n",
    "            np.where(over > threshold)[0])))  \n",
    "\n",
    "    return boxes[temp].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1997824",
   "metadata": {},
   "source": [
    "## find min distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fa8ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77432e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distance(test_path,test_):\n",
    "    save_list = []\n",
    "    for i in range(len(test_)):\n",
    "        \n",
    "        \n",
    "        if i == 0:\n",
    "            get_poi = cal_img(test_path,test_,0)\n",
    "            save = NMS(get_poi, threshold=0.3)\n",
    "            person = save\n",
    "           \n",
    "        else:\n",
    "            get_poi = cal_img(test_path,test_,i)\n",
    "            save = NMS(get_poi, threshold=0.3)\n",
    "            \n",
    "            \n",
    "            for j in range(len(save)):\n",
    "                \n",
    "                center_now = [(save[j][0] + save[j][0] + save[j][2])/2,(save[j][1] + save[j][1] + save[j][3])/2]\n",
    "                \n",
    "                min = 999999\n",
    "                \n",
    "                for k in range(len(person)):\n",
    "                    \n",
    "                    center_bf = [(person[k][0] + person[k][0] + person[k][2])/2,(person[k][1] + person[k][1] + person[k][3])/2]\n",
    "                    \n",
    "                    absdis = math.sqrt((center_now[0] - center_bf[0])**2 + (center_now[1] - center_bf[1])**2)\n",
    "                    if absdis < min:\n",
    "                        \n",
    "                        min = absdis\n",
    "                \n",
    "                save_list.append(int(min))\n",
    "            person = save\n",
    "                     \n",
    "            \n",
    "    return save_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71095593",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_li = find_distance(test_1_path,test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47de326e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "821"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(num_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35c37f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_len = [0] * (max(num_li)+1)\n",
    "x_l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "354cef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(num_li)):\n",
    "    indexadd = num_li[i]\n",
    "    list_len[indexadd] += 1\n",
    "for i in range(len(list_len)):\n",
    "    x_l.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b72b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Qt5Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc69d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    " \n",
    "plt.figure(figsize=(20, 10), dpi=100)\n",
    "\n",
    "plt.plot(x_l, list_len)\n",
    "plt.xlabel(\"distance\")\n",
    "plt.ylabel(\"number in this distance\")\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83ffecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find whether is a group\n",
    "def find_group(now_list):\n",
    "    group_p = []\n",
    "    \n",
    "    for i in range(0,len(now_list)-1):\n",
    "        center_i = [(now_list[i][0]+now_list[i][0]+now_list[i][2])/2,(now_list[i][1]+now_list[i][1]+now_list[i][3])/2]\n",
    "                \n",
    "        for j in range(i+1,len(now_list)):\n",
    "            center_j = [(now_list[j][0]+now_list[j][0]+now_list[j][2])/2,(now_list[j][1]+now_list[j][1]+now_list[j][3])/2]\n",
    "            dis = ((center_i[0]-center_j[0])**2 - (center_i[1]-center_j[1])**2)**(1/2)\n",
    "            if dis < 150:\n",
    "                group_p.append([now_list[i],now_list[j]])\n",
    "    \n",
    "    final_group = []\n",
    "    for i in range(len(group_p)):\n",
    "        if group_p[i][0][0] <= group_p[i][1][0]:\n",
    "            x = group_p[i][0][0]\n",
    "        if group_p[i][0][0] > group_p[i][1][0]:\n",
    "            x = group_p[i][1][0]\n",
    "        if group_p[i][0][1] <= group_p[i][1][1]:\n",
    "            y = group_p[i][1][1]\n",
    "        if group_p[i][0][1] > group_p[i][1][1]:\n",
    "            y = group_p[i][0][1]\n",
    "        \n",
    "        w = group_p[i][0][2] + group_p[i][1][2]\n",
    "        \n",
    "        h = group_p[i][0][3] + group_p[i][1][3]\n",
    "        \n",
    "        final_group.append([x,y,w,h])\n",
    "    \n",
    "    \n",
    "    return final_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3c9a6a",
   "metadata": {},
   "source": [
    "# For task 1 ,2 and 3. When you use this function, you need to select the area you like on the first image, so that the statistics of pedestrians in the area will be realized in the subsequent video processing. I used waitkey() to help you see each image. If you want to achieve the effect of continuous playback, you can press and hold any key to achieve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7b230ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showtask_1and2and3(test_path,test_):\n",
    "    def each_pop(a,b):\n",
    "        if len(a)!=0:\n",
    "            now = a.pop(0)\n",
    "            now_c = b.pop(0)\n",
    "            label.append(now)\n",
    "            color.append(now_c)\n",
    "            return each_pop(a,b)\n",
    "        else:\n",
    "            return\n",
    "    \n",
    "    point_size = 1\n",
    "    thickness = 2\n",
    "    label = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30]\n",
    "    color = [(130,220,255), (250,135,115), (0,255,255), (102,205,170), (138,43,226), \n",
    "         (205,51,51), (102,205,0), (238,118,33), (193,255,193), (238,18,137), \n",
    "         (3,3,3), (247,247,247), (238,162,173), (255,153,18), (61,89,171), (128,138,135), (220,20,60),\n",
    "        (0,100,0), (202,255,112), (154,50,205), (155,205,155), (148,0,211), (0,104,139),(0,100,50), (100,255,112), (154,150,205), (15,205,155), (48,0,211), (0,104,13),(0,14,139)]\n",
    "    now_person = []\n",
    "    now_number = []\n",
    "    now_color = []\n",
    "    save_two_point = []\n",
    "    save_one_point = []\n",
    "    save_two_color = []\n",
    "    save_one_color = []\n",
    "    \n",
    "   \n",
    "    for_mp4 = []\n",
    "    \n",
    "    total_person_number = 0\n",
    "    \n",
    "    select_region = cv2.imread(test_path+test_[0])\n",
    "    r = cv2.selectROI('roi',select_region , False, False )\n",
    "    \n",
    "    for i in range(len(test_)):\n",
    "        \n",
    "        region_number = 0\n",
    "        img = cv2.imread(test_path+test_[i])\n",
    "        get_poi = cal_img(test_path,test_,i)\n",
    "        save = NMS(get_poi, threshold=0.3)\n",
    "        now_person_number = len(save)\n",
    "        if len(now_person) == 0:\n",
    "            \n",
    "            now_im = cv2.rectangle(img, (r[0], r[1]), (r[0] + r[2], r[1] + r[3]), (173,255,47), 2)\n",
    "            now_im = cv2.putText(now_im,'{}'.format('region'),(r[0], r[1]),cv2.FONT_HERSHEY_SIMPLEX,0.75,(255,255,0), 2)\n",
    "            \n",
    "            for k in range(len(save)):\n",
    "                now_pont_center = [(save[k][0]+save[k][0]+save[k][2])/2,(save[k][1]+save[k][1]+save[k][3])/2]\n",
    "                \n",
    "                now_person.append(save[k])\n",
    "                now_number.append(label.pop(0))\n",
    "                now_color.append(color.pop(0))\n",
    "                save_one_point.append(now_pont_center)\n",
    "                save_one_color.append(now_color[-1])\n",
    "            \n",
    "            \n",
    "            for j in range(len(now_person)):\n",
    "                [x,y,w,h] = now_person[j]\n",
    "                now_im = cv2.rectangle(img, (x, y), (x + w, y + h), now_color[j], 2)\n",
    "            \n",
    "\n",
    "                if x+w/2 > r[0] and y + h/2 > r[1] and x+w/2 < r[0]+ r[2] and y + h/2 < r[1]+r[3]:\n",
    "                    region_number += 1\n",
    "            \n",
    "            for w in range(len(save_one_point)):\n",
    "                now_im = cv2.circle(now_im, (int(save_one_point[w][0]),int(save_one_point[w][1])), point_size, save_one_color[w], thickness)\n",
    "                \n",
    "            \n",
    "            toge = find_group(now_person)\n",
    "            toge = NMS(toge, threshold=0.3)\n",
    "            for p in range(len(toge)):\n",
    "                [x,y,w,h] = toge[p]\n",
    "                now_im = cv2.rectangle(now_im, (x, y), (x + w, y + h),(247,186,11), 2)\n",
    "            \n",
    "                \n",
    "                \n",
    "            \n",
    "            total_person_number += len(now_person) \n",
    "            cv2.putText(img, \"Count of Unique pedestrain: {}\".format(total_person_number), (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 255), 2)\n",
    "            cv2.putText(img, \"Total pedestrain: {}\".format(now_person_number), (20, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "            cv2.putText(img, \"pedestrain in region: {}\".format(region_number), (20, 140), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 0, 255), 2)\n",
    "            \n",
    "            cv2.putText(img, \"group number: {}\".format(len(toge)), (20, 190), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (100, 0, 255), 2)\n",
    "            cv2.putText(img, \"alone person: {}\".format(now_person_number-len(toge)), (20, 240), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 100, 100), 2)\n",
    "            \n",
    "            for_mp4.append(now_im)\n",
    "            \n",
    "            cv2.imshow('test',now_im)\n",
    "            cv2.waitKey()\n",
    "        else:\n",
    "            update_person = []\n",
    "            update_number = []\n",
    "            update_color = []\n",
    "            return_number = []\n",
    "            return_color = []\n",
    "            \n",
    "            # Set four rectangles on the four sides of the picture\n",
    "            highlight = []\n",
    "            right_box = [1823, 1, 97, 1079]\n",
    "            left_box = [3, 1, 123, 1079]\n",
    "            top_box = [3, 2, 1917, 70]\n",
    "            bot_box = [1, 984, 1919, 96]\n",
    "            \n",
    "            \n",
    "            # Determine whether the center point of the pedestrian border is within these four rectangular boxes\n",
    "            for k in range(len(save)):\n",
    "                now_pont_center = [(save[k][0]+save[k][0]+save[k][2])/2,(save[k][1]+save[k][1]+save[k][3])/2]\n",
    "                if now_pont_center[0] >= left_box[0] and now_pont_center[0] <= left_box[0] + left_box[2] and now_pont_center[1] >= left_box[1] and now_pont_center[1] <= left_box[1] + left_box[3]:\n",
    "                    highlight.append(save[k])\n",
    "                if now_pont_center[0] >= right_box[0] and now_pont_center[0] <= right_box[0] + right_box[2] and now_pont_center[1] >= right_box[1] and now_pont_center[1] <= right_box[1] + right_box[3]:\n",
    "                    highlight.append(save[k])\n",
    "                if now_pont_center[0] >= top_box[0] and now_pont_center[0] <= top_box[0] + top_box[2] and now_pont_center[1] >= top_box[1] and now_pont_center[1] <= top_box[1] + top_box[3]:\n",
    "                    highlight.append(save[k])\n",
    "                if now_pont_center[0] >= bot_box[0] and now_pont_center[0] <= bot_box[0] + bot_box[2] and now_pont_center[1] >= bot_box[1] and now_pont_center[1] <= bot_box[1] + bot_box[3]:\n",
    "                    highlight.append(save[k])\n",
    "             \n",
    "                \n",
    "                \n",
    "                \n",
    "                now_mid_r = save[k][0] + save[k][2]/2\n",
    "                now_mid_c = save[k][1] + save[k][3]/2\n",
    "                min = 999999999\n",
    "                for j in range(len(now_person)):\n",
    "                    bf_r = now_person[j][0] + now_person[j][2]/2\n",
    "                    bf_c = now_person[j][1] + now_person[j][3]/2\n",
    "                    if (now_mid_r-bf_r)**2+(now_mid_c-bf_c)**2 < min:\n",
    "                        min = (now_mid_r-bf_r)**2+(now_mid_c-bf_c)**2\n",
    "                        index = j\n",
    "                if min**(1/2) < 150:\n",
    "                    update_person.append(save[k])\n",
    "                    update_number.append(now_number[index])\n",
    "                    update_color.append(now_color[index])\n",
    "                    bf_point = [(now_person[index][0] + now_person[index][0] + now_person[index][2])/2,(now_person[index][1] + now_person[index][1] + now_person[index][3])/2]\n",
    "                    save_two_point.append([now_pont_center,bf_point])\n",
    "                    save_two_color.append(update_color[-1])\n",
    "                else:\n",
    "                    update_person.append(save[k])\n",
    "                    update_number.append(label.pop(0))\n",
    "                    update_color.append(color.pop(0))\n",
    "                    save_one_point.append(save[k])\n",
    "                    save_one_color.append(update_color[-1])\n",
    "                    #highlight.append(save[k])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    \n",
    "                    total_person_number += 1\n",
    "            for q in range(len(now_color)):\n",
    "                if now_color[q] not in update_color:\n",
    "                    return_number.append(now_number[q])\n",
    "                    return_color.append(now_color[q])\n",
    "                    #highlight.append(now_person[q])\n",
    "            each_pop(return_number,return_color)\n",
    "            now_person = update_person\n",
    "            now_number = update_number\n",
    "            now_color = update_color\n",
    "            \n",
    "            now_im = cv2.rectangle(img, (r[0], r[1]), (r[0] + r[2], r[1] + r[3]), (173,255,47), 2)\n",
    "            now_im = cv2.putText(now_im,'{}'.format('region'),(r[0], r[1]),cv2.FONT_HERSHEY_SIMPLEX,0.75,(255,255,0), 2)\n",
    "            \n",
    "            \n",
    "            for j in range(len(now_person)):\n",
    "                [x,y,w,h] = now_person[j]\n",
    "                now_im = cv2.rectangle(img, (x, y), (x + w, y + h), now_color[j], 2)\n",
    "            \n",
    "                if x+w/2 > r[0] and y + h/2 > r[1] and x+w/2 < r[0]+ r[2] and y + h/2 < r[1]+r[3]:\n",
    "                    region_number += 1\n",
    "            \n",
    "            for w in range(len(save_two_point)):\n",
    "                \n",
    "                now_im = cv2.line(now_im,(int(save_two_point[w][0][0]),int(save_two_point[w][0][1])),(int(save_two_point[w][1][0]),int(save_two_point[w][1][1])),save_two_color[w],thickness)\n",
    "                \n",
    "            for w in range(len(save_one_point)):\n",
    "                now_im = cv2.circle(now_im, (int(save_one_point[w][0]),int(save_one_point[w][1])), point_size, save_one_color[w], thickness)\n",
    "\n",
    "            \n",
    "            \n",
    "            toge = find_group(now_person)\n",
    "            toge = NMS(toge, threshold=0.3)\n",
    "            for p in range(len(toge)):\n",
    "                [x,y,w,h] = toge[p]\n",
    "                now_im = cv2.rectangle(now_im, (x, y), (x + w, y + h),(247,186,11), 2)\n",
    "                now_im = cv2.putText(now_im,'{}'.format('Group'),(int(x),int(y)),cv2.FONT_HERSHEY_SIMPLEX,0.5,(247,186,11), 2)\n",
    "            \n",
    "            for p in range(len(highlight)):\n",
    "                [x,y,w,h] = highlight[p]\n",
    "                now_im = cv2.rectangle(now_im, (x, y), (x + w, y + h),(255,255,0), 2)\n",
    "                now_im = cv2.putText(now_im,'{}'.format('HIGHLIGHT'),(int(x),int(y)),cv2.FONT_HERSHEY_SIMPLEX,0.75,(255,255,0), 2)\n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "            \n",
    "\n",
    "            cv2.putText(img, \"Count of Unique pedestrain: {}\".format(total_person_number), (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 255), 2)\n",
    "            cv2.putText(img, \"Total pedestrain: {}\".format(now_person_number), (20, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "            cv2.putText(img, \"pedestrain in region: {}\".format(region_number), (20, 140), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 0, 255), 2)\n",
    "            \n",
    "            cv2.putText(img, \"group number: {}\".format(len(toge)), (20, 190), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (100, 0, 255), 2)\n",
    "            cv2.putText(img, \"alone person: {}\".format(now_person_number-len(toge)), (20, 240), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 100, 100), 2)\n",
    "            \n",
    "            for_mp4.append(now_im)\n",
    "            \n",
    "            cv2.imshow('test',now_im)\n",
    "            cv2.waitKey()   \n",
    "    cv2.destroyAllWindows()\n",
    "    return for_mp4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d2d7fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19061\\AppData\\Local\\Temp\\ipykernel_920\\3934387814.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dis = ((center_i[0]-center_j[0])**2 - (center_i[1]-center_j[1])**2)**(1/2)\n",
      "C:\\Users\\19061\\AppData\\Local\\Temp\\ipykernel_920\\3934387814.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dis = ((center_i[0]-center_j[0])**2 - (center_i[1]-center_j[1])**2)**(1/2)\n",
      "C:\\Users\\19061\\AppData\\Local\\Temp\\ipykernel_920\\3934387814.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dis = ((center_i[0]-center_j[0])**2 - (center_i[1]-center_j[1])**2)**(1/2)\n",
      "C:\\Users\\19061\\AppData\\Local\\Temp\\ipykernel_920\\3934387814.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dis = ((center_i[0]-center_j[0])**2 - (center_i[1]-center_j[1])**2)**(1/2)\n",
      "C:\\Users\\19061\\AppData\\Local\\Temp\\ipykernel_920\\3934387814.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dis = ((center_i[0]-center_j[0])**2 - (center_i[1]-center_j[1])**2)**(1/2)\n",
      "C:\\Users\\19061\\AppData\\Local\\Temp\\ipykernel_920\\3934387814.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dis = ((center_i[0]-center_j[0])**2 - (center_i[1]-center_j[1])**2)**(1/2)\n",
      "C:\\Users\\19061\\AppData\\Local\\Temp\\ipykernel_920\\3934387814.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dis = ((center_i[0]-center_j[0])**2 - (center_i[1]-center_j[1])**2)**(1/2)\n"
     ]
    }
   ],
   "source": [
    "for_mp4 = showtask_1and2and3(test_1_path,test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a0ebe",
   "metadata": {},
   "source": [
    "# Convert image to video in output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e276a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 30\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video_writer = cv2.VideoWriter(filename=root+ '/output/test.mp4', fourcc=fourcc, fps=fps, frameSize=(1920,1080))  # 图片实际尺寸，不然生成的视频会打不开\n",
    "for i in range(len(for_mp4)):\n",
    "  \n",
    "    video_writer.write(for_mp4[i])\n",
    "video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad532261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:yolov5] *",
   "language": "python",
   "name": "conda-env-yolov5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
